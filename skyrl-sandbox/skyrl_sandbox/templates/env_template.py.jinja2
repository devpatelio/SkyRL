"""{{ spec.env_name.capitalize() }} environment for SkyRL-Gym."""

import re
{% if spec.parsing.method == "json_path" or spec.reward.creation.method == "json_path_rule" %}
import json
from jsonpath_ng import parse as jsonpath_parse
{% endif %}
from typing import Any, Callable, Dict, List, Optional, Tuple
from skyrl_gym.envs.base_text_env import BaseTextEnv, BaseTextEnvStepOutput
from omegaconf import DictConfig


class {{ spec.env_name.capitalize() }}Env(BaseTextEnv):
    """
    {{ spec.description or "Environment generated by SkyRL Sandbox." }}

    Environment type: {{ spec.env_type }}
    {% if spec.env_type == "multi_turn" %}Max turns: {{ spec.max_turns }}{% endif %}
    Parsing method: {{ spec.parsing.method }}
    Reward creation: {{ spec.reward.creation.method }}
    Reward scheme: {{ spec.reward.scheme.scheme }}
    """

    def __init__(
        self,
        env_config: DictConfig,
        extras: Dict[str, Any] = {},
        reward_overrides: Optional[Dict[str, Dict[str, Any]]] = None,
        llm_reward_fn: Optional[
            Callable[[str, Any, Dict[str, Any]], Dict[str, Any]]
        ] = None,
    ):
        super().__init__()
        
        # Extract ground_truth from extras (standard SkyRL-Gym pattern)
        assert "reward_spec" in extras, "reward_spec field is required"
        assert "ground_truth" in extras["reward_spec"], "ground_truth is required in reward_spec field"
        self.ground_truth = extras["reward_spec"]["ground_truth"]
        self.llm_reward_fn = llm_reward_fn

        base_creation = {{ spec.reward.creation.model_dump() }}
        base_scheme = {{ spec.reward.scheme.model_dump() }}

        reward_overrides = reward_overrides or {}
        self.reward_creation = reward_overrides.get("creation", base_creation)
        self.reward_scheme = reward_overrides.get("scheme", base_scheme)

        {% if spec.env_type == "multi_turn" %}
        self.max_turns = extras.get("max_turns", {{ spec.max_turns }})
        self.turns = 0
        {% endif %}

    def _parse_action(self, action: str) -> Any:
        """Extract answer from model output."""
        {% if spec.parsing.method == "regex" %}
        match = re.search(r"{{ spec.parsing.pattern }}", action)
        return match.group(1) if match else None
        {% elif spec.parsing.method == "json_path" %}
        try:
            data = json.loads(action)
            jsonpath_expr = jsonpath_parse("{{ spec.parsing.json_path }}")
            matches = jsonpath_expr.find(data)
            return matches[0].value if matches else None
        except (json.JSONDecodeError, Exception):
            return None
        {% endif %}

    def _evaluate_reward_signal(
        self,
        action: str,
        parsed_answer: Any,
    ) -> Tuple[bool, bool, Optional[float]]:
        """Return (is_correct, found_answer, raw_score)."""
        method = self.reward_creation.get("method", "parsed_answer_rule")
        found_answer = parsed_answer is not None
        raw_score: Optional[float] = None

        if method == "parsed_answer_rule":
            rule_type = self.reward_creation.get("rule_type", "exact_match")
            if rule_type == "exact_match":
                is_correct = (
                    parsed_answer is not None
                    and str(parsed_answer).strip() == str(self.ground_truth).strip()
                )
            elif rule_type == "regex_match":
                pattern = self.reward_creation.get("regex_pattern") or str(
                    self.ground_truth
                )
                is_correct = (
                    parsed_answer is not None
                    and re.match(str(pattern), str(parsed_answer)) is not None
                )
            elif rule_type == "numeric_tolerance":
                tolerance = float(self.reward_creation.get("numeric_tolerance") or 0.01)
                try:
                    if parsed_answer is not None:
                        answer_num = float(parsed_answer)
                        ground_truth_num = float(self.ground_truth)
                        is_correct = abs(answer_num - ground_truth_num) <= tolerance
                    else:
                        is_correct = False
                except (ValueError, TypeError):
                    is_correct = False
            else:
                is_correct = False
        elif method == "json_path_rule":
            is_correct = False
            found_answer = False
            json_path = self.reward_creation.get("json_path")
            if not json_path:
                return False, False, None
            try:
                data = json.loads(action)
                jsonpath_expr = jsonpath_parse(str(json_path))
                matches = jsonpath_expr.find(data)
                if matches:
                    value = matches[0].value
                    found_answer = True
                    success_values = self.reward_creation.get("json_success_values") or []
                    threshold = self.reward_creation.get("json_threshold")
                    if success_values:
                        is_correct = value in success_values
                    elif threshold is not None:
                        try:
                            raw_score = float(value)
                            is_correct = raw_score >= float(threshold)
                        except (TypeError, ValueError):
                            is_correct = False
            except Exception:
                is_correct = False
        elif method == "llm_verifier":
            found_answer = True
            if not self.llm_reward_fn:
                raise RuntimeError(
                    "llm_reward_fn must be provided when using llm_verifier rewards"
                )
            judge_result = self.llm_reward_fn(action, self.ground_truth, self.reward_creation)
            is_correct = bool(judge_result.get("is_correct"))
            judge_score = judge_result.get("score")
            raw_score = float(judge_score) if judge_score is not None else None
        else:
            is_correct = False

        return is_correct, found_answer, raw_score

    def _apply_scheme(
        self,
        is_correct: bool,
        found_answer: bool,
        raw_score: Optional[float],
    ) -> float:
        scheme = self.reward_scheme
        scheme_type = scheme.get("scheme", "binary")

        if scheme_type == "dense" and raw_score is not None:
            return float(raw_score)

        if is_correct:
            return float(scheme.get("correct_reward", 1.0))

        if not found_answer:
            return float(
                scheme.get("format_error_reward", scheme.get("incorrect_reward", 0.0))
            )

        if scheme.get("partial_reward") is not None and scheme_type in {"partial", "dense"}:
            return float(scheme["partial_reward"])

        return float(scheme.get("incorrect_reward", 0.0))

    def _calculate_reward(self, action: str, parsed_answer: Any) -> Dict[str, Any]:
        is_correct, found_answer, raw_score = self._evaluate_reward_signal(
            action, parsed_answer
        )
        reward = self._apply_scheme(is_correct, found_answer, raw_score)
        return {
            "reward": reward,
            "is_correct": is_correct,
            "found_answer": found_answer,
            "raw_score": raw_score,
        }

    def step(self, action: str) -> BaseTextEnvStepOutput:
        """Execute one step in the environment."""
        {% if spec.env_type == "multi_turn" %}
        self.turns += 1
        {% endif %}

        answer = self._parse_action(action)
        reward_info = self._calculate_reward(action, answer)

        {% if spec.env_type == "single_turn" %}
        # Single-turn environment always ends after one step
        return BaseTextEnvStepOutput(
            observations=[],
            reward=reward_info["reward"],
            done=True,
            metadata={
                "parsed_answer": answer,
                "raw_score": reward_info["raw_score"],
                "is_correct": reward_info["is_correct"],
            },
        )
        {% elif spec.env_type == "multi_turn" %}
        # Determine if episode is done
        {% if spec.done_condition == "always_single_step" %}
        done = True
        {% elif spec.done_condition == "max_turns_only" %}
        done = self.turns >= self.max_turns
        {% elif spec.done_condition == "correct_or_max_turns" %}
        done = self.turns >= self.max_turns or reward_info["is_correct"]
        {% endif %}

        if done:
            return BaseTextEnvStepOutput(
                observations=[],
                reward=reward_info["reward"],
                done=True,
                metadata={
                    "parsed_answer": answer,
                    "raw_score": reward_info["raw_score"],
                    "is_correct": reward_info["is_correct"],
                    "turns": self.turns,
                },
            )

        # Provide feedback for another attempt
        if reward_info["found_answer"] and not reward_info["is_correct"]:
            feedback = """{{ spec.feedback.on_incorrect }}""".format(answer=answer)
        else:
            feedback = """{{ spec.feedback.on_format_error }}"""

        return BaseTextEnvStepOutput(
            observations=[{"role": "user", "content": feedback}],
            reward=0.0,
            done=False,
            metadata={
                "parsed_answer": answer,
                "raw_score": reward_info["raw_score"],
                "turns": self.turns,
            },
        )
        {% endif %}

